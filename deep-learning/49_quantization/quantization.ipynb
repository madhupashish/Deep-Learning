{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue\" align=\"center\">Quantization Tutorial</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization is a technique to downsize a trained model so that you can deploy it on EDGE devices. In this tutorial we will,\n",
    "\n",
    "(1) Train a hand written digits model\n",
    "\n",
    "(2) Export to a disk and check the size of that model\n",
    "\n",
    "(3) Use two techniques for quantization (1) post training quantization (3) quantization aware training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train) , (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x255fe7bb760>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flattened = X_train.reshape(len(X_train), 28*28)\n",
    "X_test_flattened = X_test.reshape(len(X_test), 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_flattened.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style='color:purple'>Using Flatten layer so that we don't have to call .reshape on input dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2734 - accuracy: 0.9226\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1245 - accuracy: 0.9635\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0866 - accuracy: 0.9745\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0652 - accuracy: 0.9800\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0516 - accuracy: 0.9847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25bbb173820>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0888 - accuracy: 0.9708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08878576755523682, 0.97079998254776]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"./saved_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style='color:blue'>(1) Post training quantization</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Without quantization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"./saved_model\")\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With quantization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"./saved_model\")\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read this article for post training quantization: https://www.tensorflow.org/model_optimization/guide/quantization/post_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "319792"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84752"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see above that quantizated model is 1/4th the size of a non quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tflite_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tflite_quant_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have above files saved to a disk, check their sizes. Quantized model will be obvi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style='color:blue'>(2) Quantization aware training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer (QuantizeLaye (None, 28, 28)            3         \n",
      "_________________________________________________________________\n",
      "quant_flatten (QuantizeWrapp (None, 784)               1         \n",
      "_________________________________________________________________\n",
      "quant_dense (QuantizeWrapper (None, 100)               78505     \n",
      "_________________________________________________________________\n",
      "quant_dense_1 (QuantizeWrapp (None, 10)                1015      \n",
      "=================================================================\n",
      "Total params: 79,524\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 14\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "# q_aware stands for for quantization aware.\n",
    "q_aware_model = quantize_model(model)\n",
    "\n",
    "# `quantize_model` requires a recompile.\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0438 - accuracy: 0.9866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x255fe86ba30>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_aware_model.fit(X_train, y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0802 - accuracy: 0.9755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08016839623451233, 0.9754999876022339]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_aware_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as flatten_layer_call_fn, flatten_layer_call_and_return_conditional_losses, dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn while saving (showing 5 of 15). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\dhava\\AppData\\Local\\Temp\\tmpqnsx4bvx\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\dhava\\AppData\\Local\\Temp\\tmpqnsx4bvx\\assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "tflite_qaware_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82376"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tflite_qaware_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tflite_qaware_model.tflite\", 'wb') as f:\n",
    "    f.write(tflite_qaware_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
